import nltk
from konlpy.tag import Okt
from collections import Counter
import re
from .utils import *
import json
import pandas as pd
from pathlib import Path
from tqdm import tqdm

def process_jsonl_to_csv(input_filename, output_filename, label_field="label"):
    input_path = Path("data") / input_filename
    output_path = Path("cal") / output_filename

    data = []

    # ë¨¼ì € ë¼ì¸ ìˆ˜ ë¯¸ë¦¬ ë¡œë”© (tqdmì— ì‚¬ìš©í•˜ê¸° ìœ„í•´)
    with open(input_path, "r", encoding="utf-8") as f:
        lines = f.readlines()

    for line in tqdm(lines, desc=f"ğŸ” {input_filename} ì²˜ë¦¬ ì¤‘"):
        item = json.loads(line)
        text = item.get("text") or item.get("content") or item.get("body")
        label = item.get(label_field)

        if not text:
            continue

        features = extract_features(text)
        features["label"] = label
        data.append(features)

    df = pd.DataFrame(data)

    if output_path.suffix == ".xlsx":
        df.to_excel(output_path, index=False)
    else:
        df.to_csv(output_path, index=False, encoding="utf-8-sig")

    print(f"âœ… ì €ì¥ ì™„ë£Œ: {output_path}")

def extract_features(text):
    return {
        "avg_sentence_length": cal_avg_sentence_length(text),
        "ttr": calculate_ttr(text),
        "pos_ngram_diversity": pos_ngram_diversity(text, n=4),
        "comma_inclusion_rate": comma_inclusion_rate(text),
        "avg_comma_usage_rate": avg_comma_usage_rate(text),
        "avg_comma_relative_position": avg_comma_relative_position(text),
        "avg_segment_length": avg_segment_length(text),
        "pos_diversity_around_commas": pos_diversity_around_commas(text),
        "repeated_morpheme_ratio": repeated_morpheme_ratio(text),
        "avg_commas_per_sentence": avg_commas_per_sentence(text),
    }


# korean_text_sample = "ì¥í¬ëŠ” ì¥ì¹˜ë¼ëŠ” ì‘ì€ ë¬¼ê³ ê¸°ë¥¼ ì¡ì•„ ì “ê°ˆë¡œ ë§Œë“  í›„, ì¡°ë¯¸ê°„ë‹¨ì¡°ë¦¬ê±´ì¡°í•´ì„œ ë§Œë“  ìŒì‹ì…ë‹ˆë‹¤. ì¥ì¹˜íšŒëŠ” ì¡ì–´ë¨¹ëŠ” ê²ƒìœ¼ë¡œ ìì—°ì‚°íšŒì´ë©°, ë§¤ìš° ë§›ìˆëŠ” ìŒì‹ì…ë‹ˆë‹¤. ì¥í¬ì˜ ê»ë°ê¸°ë¥¼ ë²—ê¸´ í›„ì—ëŠ”, ì–‘ì´ ì ì–´ êµ¬ì´ë¡œ ë¨¹ì„ë§Œí•œ ì–‘ì´ ì•„ë‹™ë‹ˆë‹¤. ê·¸ëŸ¬ë‚˜ ì¥ì¹˜ë¥¼ ìŒì‹ìœ¼ë¡œ ë¨¹ì„ ìˆ˜ëŠ” ìˆìŠµë‹ˆë‹¤. ì¥í¬ì˜ ê»ë°ê¸°ë¥¼ ë²—ê¸°ë©´ ì§€ëŠëŸ¬ë¯¸ ë¶€ë¶„ë§Œ ë‚¨ê²Œ ë˜ë©°, ì´ ë¶€ë¶„ì€ íšŒë‚˜ ì¡°ë¦¼ìœ¼ë¡œ ë¨¹ì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤."

# data = extract_features(korean_text_sample)
    
# print(data)

#process_jsonl_to_csv("ai_data.train.jsonl", "ai.train.xlsx")
process_jsonl_to_csv("ai_data.valid.jsonl", "ai.valid.xlsx")
process_jsonl_to_csv("ai_data.test.jsonl", "ai.test.xlsx")

process_jsonl_to_csv("human_data.train.jsonl", "human.train.xlsx")
process_jsonl_to_csv("human_data.valid.jsonl", "human.valid.xlsx")
process_jsonl_to_csv("human_data.test.jsonl", "human.test.xlsx")