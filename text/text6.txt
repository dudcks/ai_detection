1. 적합한 알고리즘을 위한 손실함수 정규화


위의 그래프를 보면 학습을 반복할 때, training data에 대한 loss는 감소하지만, 오히려 과적합되어 validation data에서는 loss가 증가하는 모습을 보인다. 이 경우 단 하나의 목표는 손실을 최소하는 것이기 때문에, 모델이 얼마나 복잡해지는가는 상관하지 않는다. 아래의 식은 이를 나타내는 경험적위험최소화 의 식이다.


데이터의 여러 특성값들을 모두 고려하여 학습하게 되면 혼자 튀는 특성값까지 하나 하나 다 챙기는 모델이 나오고, 이는 오히려 새로운 데이터로 검증했을 때 정확도가 떨어지게 된다. 따라서 손실은 최소화하면서 모델의 복잡성은 줄이기 위해 정규화를 사용한다. 이를 위해 모델복잡도를 측정하는 정규화항을 손실항에 더해 최소화하도록 한다.

**모델의 복잡성을 낮추기 위해서는 두가지 측정방법이 있는데, 이 글에서는 그 중 하나인 모든 특성의 가중치에 대한 함수로서의 복잡도를 다룬다.

L2 정규화
여기서 complexity(Model)에는 여러가지 정규화식이 사용될 수 있는데, 우선 L2정규화 공식을 사용해 이해해보자.

최소화하고자 하는 함수에 모든 특성의 가중치제곱합이 들어가게 되면서, 특성들의 가중치들의 복잡성 또한 적절히 최소화할 수 있게 되었다. 제곱을 함으로서 얻는 효과는  불필요한 이상치 (극단적으로 다른값) 가중치의 영향을 크게 받음으로서 해당특성의 가중치를 최소화하는 방향으로 가중치를 조정하여 모델 가중치를 0에 가깝게 만든다. 


람다 λ
여기서 우리는 적당히 손실도 적고, 복잡성도 낮은 모델을 만들기 위해 어느정도로 정규화를 해야하는지 의문이 든다. 이를 위해 람다라는 스칼라값을 적용한다.

람다 : 얼마나 정규화를 할지 조정하는 정규화율 (Regularization rate)


람다를 높이면 왼쪽처럼 정규화효과가 강화되어 가중치들이 평균으로 유도된다.  각각의 장단점이 있다.

람다값이 높을 때 : 이상치값들이 무시되어 모델은 단순해지지만, 모델이 과소적합하여 training data에 대해 충분한 학습을 하지 못 할 수 있다.

람다값이 낮을 때 : 이상치값들을 너무 많이 학습해 모델은 복잡해지고, 데이터가 과적합하여 새로운 데이터로 일반화하지 못 할 수 있다.

 
2. 정규화공식의 이해 (L1 , L2 regularization)
사실 적당히 이해하고 넘어가려면 하겠는데 제대로 이해하기 위해서는 정말 많은 개념이 필요했다...

light-tree.tistory.com/125


딥러닝 용어 정리, L1 Regularization, L2 Regularization 의 이해, 용도와 차이 설명

제가 공부하고 정리한 것을 나중에 다시 보기 위해 적는 글입니다. 제가 잘못 설명한 내용이 있다면 알려주시길 부탁드립니다. 사용된 이미지들의 출처는 본문에 링크로 나와 있거나 글의 가장

light-tree.tistory.com
⬆ 위의 글이 정말 설명이 잘 되어있다.

L1 Norm 과 L2 Norm
Norm 이란 벡터의 길이/크기를 측정하는 함수이다. 하나의 벡터 안에는 차원에 따라 여러개의 원소들이 들어 있는데, 이 벡터의 크기를 측정하는 방법이 다양하고, 그 중 L1과 L2 방법에 대해 알아보자.

L1 Norm

L2 Norm
L1 Norm 은 각 원소의 차이를 단순히 더한 것이고, L2 Norm은 두점간의 거리를 구할때처럼 최소거리를 구하는 식이다.

왼쪽 그림에서 초록색 선은 L2 Norm이고, 나머지선들은 L1 Norm이라고 생각할 수 있다.

두 점을 연결하는 거리를 구할 때, 흰색건물을 무시하고 가장 빠른거리로 갈 수 있다면, 초록선이 가장 빠른길이 될 것이다.

하지만, 건물을 피해 길로만 가야한다면, 빨간길, 파란길, 노란길 모두 길은 다르지만 거리는 같을 것이다. 
이를 다음의 예에서 다시 이해해보자.

즉 L1 은 벡터 b와 같이 특정 feature(3,4번째 원소)가 없어도 벡터 a와 같은 값이 나오게 되는 반면, L2는 상대적으로  feature들을 모두 고려하여 unique한 결괏값을 띠게 된다.

이러한 Norm의 개념을 이용하여 정규화에 적용한 L1 loss와 L2 loss 의 식은 다음과 같다.

L1 loss vs L2 loss

좌 : L1 loss    우 : L2 loss
여기서 y_i는 실제데이터값이고, f(xi)는 예측함수에 데이터의 x_i라는 특성값을 넣어 얻은 예측값이고, 둘 사이의 오차를 구하는 공식이다. 상대적으로 제곱을 사용한 L2가 혼자 튀는 이상치(outliers)의 영향을 크게 받을 것이고, 이를 적당히 무시하고자 한다면 L1 loss를 사용하는 경우가 많다.

L1 regularization vs L2 regularization
 

왼쪽 : L1 정규화  오른쪽 : L2 정규화
 
알고리즘을 최적화하기 위해 손실을 최소화하기 위한 비용함수 Cost에 모델의 복잡성을 고려하기 위한 가중치를 L1과 L2 정규화식에 따라 추가하였다. 이에 따라 가중치가 커지는 것을 방지할 수 있다.  람다 아래에 2로 나누는 것은 논문마다 다르다는데 그냥 공식으로 받아들이고 가면 될듯 (ㅠㅠ)

🍎 결론
L1 은 상대적으로 덜 중요한 특성값을 무시할 수 있어서 sparse feature에 의존한 모델에서 불필요한 feature의 가중치를 정확히 0으로 만들어 피처를 모델이 무시하도록 할 수 있다.

L2 는 혼자 튀는 이상치에 대한 가중치를 0에 가깝게 만들어 모델의 복잡성을 줄일 수 있으나 L1처럼 정확히 0으로 만들어 feature를 아예 무시할 수는 없다. 

 